## 构建第一个神经网络
------

1 关系拟合 (回归)

**构建数据**

我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: y = a * x^2 + b, 我们给 y 数据加上一点噪声来更加真实的展示它.

**建立神经网络** 

建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(__init__()), 然后再一层层搭建(forward(x))层于层的关系链接. 

**训练网络**
训练网络模型 学习模型参数

**可视化训练过程** 
为了可视化整个训练的过程, 更好的理解是如何训练
 
[具体代码参考](./Net.py)
 
 
2 区分类型 (分类)
通过神经网络来区分节点类型。

**构建数据**
我们创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.

**建立神经网络** 
建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(__init__()), 然后再一层层搭建(forward(x))层于层的关系链接. 这个和我们在前面 regression 的时候的神经网络基本没差. 

**训练网络**
训练网络模型 学习模型参数

**可视化训练过程** 
为了可视化整个训练的过程, 更好的理解是如何训练
 
[具体代码参考](./classicNet.py)

3 快速搭建模型
Torch 中提供了很多方便的途径, 同样是神经网络, 能快则快, 我们看看如何用更简单的方式搭建同样的回归神经网络.

**快速搭建** 
可以使用  torch.nn.Sequential 将网络模型的每部分加入其中，快速得到模型结构

[具体代码参考](./Sequential.py)

4 保存提取
训练好了一个模型, 我们当然想要保存它, 留到下次要用的时候直接提取直接用。

**保存模型**
使用 torch.save() 将训练好的模型参数保存起来 


**获取模型**
使用 **torch.load** 保存好的模型参数加载进网络中 

[具体代码参考](./Save.py)


5 批训练
Torch 中提供了一种帮你整理你的数据结构的好东西, 叫做 DataLoader, 我们能用它
来包装自己的数据, 进行批训练. 而且批训练可以有很多种途径

[具体代码参考](./batch_train.py)


6 加速神经网络训练 (Speed Up Training)

包括以下几种模式:

* Stochastic Gradient Descent (SGD)
* Momentum
* AdaGrad
* RMSProp
* Adam

越复杂的神经网络 , 越多的数据 , 我们需要在训练神经网络的过程上花费的时间也就越多. 
原因很简单, 就是因为计算量太大了. 可是往往有时候为了解决复杂的问题, 复杂的结构和大数据又是不能避免的, 
所以我们需要寻找一些方法, 让神经网络聪明起来, 快起来.

7 Optimizer 优化器
对上一节提到的4中优化器的实践。

SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 
它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 
而 Adam 又是 RMSprop 的升级版. 
不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 
所以说并不是越先进的优化器, 结果越佳. 我们在自己的试验中可以尝试不同的优化器, 
找到那个最适合你数据/网络的优化器.

[具体代码参考](./Optimizer_test.py)